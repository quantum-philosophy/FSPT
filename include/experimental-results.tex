\input{include/assets/tables/target}
This section illustrates the performance of the toolchain presented in
\sref{toolchain}. All experiments are conducted on a \sc{GNU}/Linux machine
equipped with 16 processors Intel Xeon E5520 2.27~\sc{GH}z and 24~\sc{GB} of
\sc{RAM}.

To begin with, we would like to describe how we acquired reference traffic and
workload data for our experiments; the two were discussed in \sref{traffic} and
\sref{workload}, respectively. This information might serve as a set of
guidelines for potential users of the proposed methodology and toolbox.

In order to obtain real-life traffic patterns, we used a dataset published by
Google \cite{google}. The dataset contains usage data of a computer cluster over
a month period, namely, over May 2011. We downloaded the table tracking the life
cycles of the jobs submitted to the cluster and extracted the time stamps of the
first event related to each job. As a result, we obtained around 670,000 data
points, which we used for model fitting as it was described in \sref{traffic}.
Needless to mention that one can query the data in other ways in order to
distill traffic patterns that are more relevant to particular problems.

Regarding workload patters, we made use of the benchmark suites that are
commonly utilized in research nowadays. With our toolbox in place, this approach
is even natural as Sniper provides a smooth integration with some of the most
popular benchmark suites out of the box. In our experiments, the workload
patterns were obtained by simulating and recording (via our recording
infrastructure displayed in \fref{recorder}) the programs from the popular
\sc{PARSEC} \cite{bienia2011} and \sc{SPEC CPU2006} \cite{cpu2006} benchmark
suites; the former contains 13 programs, and the latter 29 programs. The
architecture used in these simulations is outlined in \tref{target}, which
corresponds to Intel's Nehalem-based Gainestown series. Many programs can be
executed with inputs of different sizes. Sniper also makes it easy to work with
arbitrary programs and experiment with different x86-based architecture setups.

All reference data that we collected and processed in order to make them
suitable for our toolbox are available online at \cite{sources}, which also
contains a detailed description of the target architecture used for the
recording of workload patterns.

\subsection{Recording}
\input{include/recording}

\subsection{Streaming}
\input{include/streaming}

To summarize, we have discussed the performance of Recorder and Streamer. The
results reported in \tref{recording} motivate our work and communicate well the
message of this paper: the speed of the state-of-the-art simulators is severely
onerous for the purpose of experimenting with data-driven techniques. The core
problem is that such techniques typically require lots of data (long execution
traces); moreover, these data might need to be recalculated each time a
parameter changes (for instance, the parameterization of the scheduling policy).
The results in \tref{streaming} show that the proposed approach can efficiently
tackle this problem by taking the data burden away and, hence, making it easier
to experiment with data-driven techniques.
