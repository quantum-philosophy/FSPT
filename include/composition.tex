As a result of the previous two subsections, we have obtained a stream of jobs.
This streams needs to be processed, which is the topic of this subsection. Here,
\emph{processing} refers to progressively building a schedule,\footnote{In this
paper, the mapping of tasks to processing elements is assumed to be a part of
the scheduling procedure.} constructing a power profile, and computing the
corresponding temperature profile. In \fref{methodology}, this functionality
resides in the box labeled ``Streamer.''

As motivated in \sref{motivation}, a prominent use case of our methodology is
the development of management strategies. Therefore, the management strategy
(including the scheduling policy) of the system at hand is devised by the user,
which is further detailed in \sref{usage-schemes}. Consequently, a scheduling
policy is given, and it decides on a schedule. Namely, for each arrived job, the
policy specifies when the job gets access to certain resources. Then we add
accordingly the recorded power pattern of the job to the power profile of the
system. A helpful analogy here might be paving a road with rectangular bricks of
different sizes. As the time goes by, we feed the power profile to a temperature
simulator and obtain a temperature profile.

The obtained power and temperature profiles are the final output of our
methodology. The key observation to be made is that no expensive performance or
power simulations are involved in the data-synthesis stage. The time consumed by
the procedure delineated above is practically negligible.

At this point, we would like to draw attention to the following. It should be
clear that, by recording power directly, we make a trade-off, and making this
trade-off is necessary. Many details pertaining to programs' executions have
been discarded in order to gain speed. What has been baked into recordings
cannot be to altered at the data-synthesis stage in general. Consider, for
instance, a recording of a program that had two cores at its disposal. This
pattern cannot be used to replay the execution as if there was only one core.
Similarly, the recording cannot tell what would happen if the program could
leverage one additional core. Another limitation concerns resource sharing,
which is twofold. First, workload patterns obtained in isolation cannot be used
to fabricate the interleaving of two programs (time sharing). Second, even if
two programs run on two different cores, they still can affect each other by
competing for such resources as shared L3 caches. Currently, the above concerns
can be addresses in our methodology only partially.
