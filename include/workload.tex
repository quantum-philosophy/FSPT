\input{include/assets/figures/streams}
In the previous subsection, we introduced our approach to generating streams of
arrivals; however, an arrival is only a time stamp without any information about
the actual workload. In this subsection, we describe how workload candidates are
obtained and utilized for substantiating job arrivals, which is depicted at the
bottom of \fref{methodology}. Figure~\ref{fig:streams} might also be helpful to
clarify the relation between the previous and current sections. Let us begin by
noting that workload candidates should conform to a number of criteria:
\begin{itemize}
  \item {\bfseries Requirement~1.} Workloads should represent well the
  applications or services that the system in question is supposed to provide to
  the end user.

  \item {\bfseries Requirement~2.} A workload should be fast to evaluate, which,
  in our context, refers to obtaining the power consumption over time of that
  workload.
\end{itemize}
As emphasized in \sref{problem-formulation}, we aim to produce realistic power
and temperature traces, and we would like to do it fast. The above requirements
are a consequence of this goal.

Our workload modeling is based on full-system simulations of reference programs.
However, if we had incorporated such simulations into our workflow
\emph{directly}, we would have wound up with a configuration similar to the one
displayed in \fref{development}. This, of course, would have defeated the
purpose of our work since, as motivated in \sref{simulation}, detailed
simulations are too time consuming. Instead, we propose the use of high-level
recordings; this functionality corresponds to the modules labeled ``Recorder''
in \fref{methodology}. To elaborate, using an adequate simulator capable of
modeling the target architecture, we execute each reference program in isolation
and record certain information about this execution.\footnote{Such a technique
is similar in spirit to PinPlay \cite{patil2010}, which is a tool for recording
and replaying an execution of a program at the instruction level.} At a later
stage of our pipeline (see the Streamer module in \fref{methodology}), the
collected information is utilized in order to flesh out jobs upon their arrival,
and this stage requires no simulation. These recordings are building blocks:
they are combined to form complex workloads corresponding to multiple programs
running in parallel, which will be elaborated on in \sref{composition}.

From our experience, performance and power simulation takes by far the largest
expense in terms of time. Therefore, the information about a reference program's
execution that we propose to record is the power consumption of that execution.
This approach pushes the aforementioned expense to the data-acquisition stage
and eliminates it all together from the data-synthesis stage. To put it
differently, each recording is obtained via a one-time simulation at the
data-acquisition stage, and from there on the recording is reused as many times
as needed at the data-synthesis stage of our methodology. Since the actual data
generation, which takes place at the data-synthesis stage, is deliver from the
expensive simulations, it has a very low computational demand. This demand is
negligible compared to the one of the scenario depicted in \fref{development},
in which one undertakes performance and power simulations nonstop.

The power consumption of a program can be recorded differently; let us be more
specific about what we do. The first aspect to note is that we record power as a
function of time (assuming a certain sampling interval). Second, the dynamic and
static components of the power consumption are recorded separately in order to
get a better control over the subsequent composition (\sref{composition}).
Third, the power consumption is recorded for all processing elements that are
relevant to the subsequent study (for instance, cores and shared caches).

The result of the recording procedure is a repository of power traces
corresponding to real programs, which we refer to as workload patterns; see the
clouds in \fref{methodology} and \fref{streams}.

Full-system simulations obviously take time; however, as mentioned previously,
they should be done only once. Moreover, since researchers tend to test their
ideas using similar sets of benchmark suites, such as \sc{PARSEC}
\cite{bienia2011} and \sc{SPEC CPU2006} \cite{cpu2006}, and considering similar
sets of target architectures, it is reasonable to create a common repository of
power patterns that will be populated and maintained online by the research
community. The repository could be structured in a way that would allow for a
proper accommodation of different recording conditions. Having established such
a common repository, power patterns will be at a one-click distance from any
single researcher, and no prior simulation will be needed. The role of the
repository could be similar to the one played nowadays by the benchmark suites
themselves, but it would be on a different level of abstraction and for a
different purpose.

Let us now turn to the question: How do we choose which workload pattern to use
for a particular arrival? First of all, since arrivals are modeled as a stream,
it is convenient to turn workloads into a stream as well so that we can draw an
element from each stream and obtain a fully characterized job (time and
payload). Then the question boils down to the logic behind the workload stream.
The workload stream can be dependent on the arrival stream, which allows for
modeling, for instance, periodic workloads. The workload stream can also be
autocorrelated, which allows for modeling, for instance, coupled workloads.
Since such aspects rely heavily on domain-specific knowledge, we refrain from
providing any particular dependency model. The default option used in our
toolchain, which is to be discussed in \sref{toolchain}, is drawing samples from
a uniform distribution over a set of workload patterns.

The stream abstraction introduced in the previous paragraph also helps to
communicate the notion of workload transformation. A workload transformation is
an optional operation that can be used for workload diversification and for
emulation of the effects of such techniques as dynamic voltage/frequency
scaling. A selected workload pattern can pass through a number of filters---such
as noise, offset, scale, stretch, and shrink---before being pulled from the
stream. This, however, should be done thoughtfully as the purpose of recording
power patterns is in part to bring in realistic power fluctuations.

To recapitulate, we have obtained a database of reference workloads and
discussed the formation of workload streams, which complement arrival streams.
The patterns correspond to executions of real programs and, thus, exhibit
realistic traits.
