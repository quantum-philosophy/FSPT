In the previous subsection, we introduced our approach to synthesizing arrival
times, capturing the properties of real arrival data such as burstiness,
self-similarity, and long-range dependence. Now we need to associate a concrete
workload with each arrival time or, equivalently, with each job or user request.
In this regard, there are two main aspects to discuss: the set of workload
candidates and the decision rule used to select a particular candidate for a
particular arrival.

Let us discuss workloads first. Keeping in mind the goal of this work, workloads
should conform to a number of criteria. First, as emphasized throughout the
paper, we aim to produce realistic power and temperature traces; consequently,
the workloads should represent well the applications/services that the system is
supposed to provide to the end user. Second, a workload should be fast to
evaluate, which, in our context, refers to computing the power consumption of
that workload.

The particularities of the power consumption of a computer program are hard to
fabricate. A sequence of random numbers taken out of thin air will not do the
trick as programs have certain algorithmic structures. A program might, for
instance, traverse a number of phases, and each phase might trigger a number of
distinctive computations, shaping the corresponding power and temperature
profiles. Such features are important to preserve in order to make the
subsequent experimentation with machine-learning techniques and alike
meaningful.

With the above concern in mind, the workload-modeling part of our methodology is
based on full-system simulations of reference programs. However, if we had
incorporated such simulations directly into our workflow, it would have defeated
the purpose of our work since, as motivated in \sref{introduction}, detailed
simulations are too time consuming. Instead, we propose the use of high-level
recordings. To elaborate, using an adequate simulator capable of modeling the
target architecture, we execute each reference program and record certain
information about this execution.\footnote{Such a technique is similar in spirit
to PinPlay \cite{patil2010}, which is a tool for recording and replaying an
execution of a program on the instruction level.} At a later stage of our
pipeline, the collected information is utilized in order to flesh out jobs upon
their arrival, and this stage requires no simulation.

From our experience, performance and power simulation is by far the largest
expense on the way to temperature, and, therefore, we propose to record power
directly, eliminating this expense all together. Then the result of the
procedure described above is a repository of power traces corresponding to real
programs, which we shall refer to as power patterns.

Full-system simulations obviously take time; however, they should be done only
once. Moreover, since researchers tend to test their ideas using similar sets of
benchmark suites and considering similar sets of target architectures, it makes
sense to create a common repository of power patterns that will be populated and
maintained online by the research community. In this case, power patterns will
be at a one-click distance from any single researcher, and no prior simulation
will be needed. The role of such a repository could be similar to the one played
nowadays by the benchmark suites themselves, but it would be on a different
level and for a different purpose.

Now, it should be clear that, by recording power directly, we make a trade-off:
many details related to programs' executions are discarded in order to gain
speed. What has been baked in into recordings cannot be to altered at the usage
stage in general. Consider, for instance, a recording of a program that had two
cores and a shared L3 cache at its disposal. This pattern cannot be used to
replay the execution as if there was only one core. Similarly, the recording
cannot tell what would happen if the program could leverage one extra core.

In our experiments, we use the applications from two benchmark suites, namely,
from \sc{PARSEC} \cite{bienia2011} and \sc{SPEC CPU2006} \cite{cpu2006}.
